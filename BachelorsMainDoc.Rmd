---
title: "Bachelors Main Document"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

knitr::knit_engines$set(python = reticulate::eng_python)
```


```{r, Packages}

library(pacman)
p_load(
  reshape2,
  jsonlite,
  tidyverse,
  tm,
  topicmodels,
  parallel,
  tidytext,
  plotly,
  widyr,
  data.table,
  Morpho,
  pracma,
  plotly,
  lubridate,
  tidyr,
  LDAvis,
  servr,
  igraph,
  ggraph,
  gensimr,
  text2vec
)

```

```{r}
df <- readRDS("full_tokenized_data.rds")

stop_ord <- read_csv("custom_stop_words.csv")

table_df<- df %>%
    filter(upos %in% c("VERB", "NOUN", "ADV", "PROPN", "ADJ")) %>%
    anti_join(stop_ord) %>%
    count(lemma, CoronaStatus) %>%
    drop_na()
```

```{r, tfidf}

textLemmas <- df %>%
  drop_na() %>%
    filter(ner == "O") %>% 
  count(CoronaStatus,lemma, sort = T)

totalLemmas <- textLemmas %>% 
  drop_na() %>% 
  group_by(CoronaStatus) %>% 
  summarise(total = sum(n))

tfidf_df <- left_join(textLemmas,totalLemmas)

tfidf_df <- tfidf_df %>% 
  bind_tf_idf(lemma, CoronaStatus, n)

```


```{r, tfidf plot}
tfidf_df %>%
  group_by(CoronaStatus) %>%
  top_n(20, tf_idf) %>%
  ungroup() %>%
  mutate(CoronaStatus = as.factor(CoronaStatus),
         lemma = reorder_within(lemma, tf_idf, CoronaStatus)) %>%
  ggplot(aes(lemma, tf_idf, fill = CoronaStatus)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap( ~ CoronaStatus, scales = "free_y") +
  coord_flip() +
  ggtitle("TF-IDF: NER-filtered") +
  scale_x_reordered() +
  scale_y_continuous(n.breaks = 3)
```
```{r}
textLemmasNames <- df %>%
  drop_na() %>% 
  filter(ner == "B-PER" | ner == "I-PER") %>% 
  count(CoronaStatus,lemma, sort = T)

totalLemmasNames <- textLemmasNames %>% 
  drop_na() %>% 
  group_by(CoronaStatus) %>% 
  summarise(total = sum(n))

tfidf_dfNames <- left_join(textLemmasNames,totalLemmasNames)

tfidf_dfNames <- tfidf_dfNames %>% 
  bind_tf_idf(lemma, CoronaStatus, n)

tfidf_dfNames %>%
  arrange(desc(tf_idf)) %>%
  mutate(lemma = factor(lemma, levels = rev(unique(lemma)))) %>% 
  group_by(CoronaStatus) %>% 
  top_n(20) %>% 
  ungroup() %>%
  ggplot(aes(lemma, tf_idf, fill = CoronaStatus)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~CoronaStatus, scales = "free") +
  coord_flip()+
  ggtitle("TF-IDF: Names")
```


```{r}
FuncLDA <- function(dataframe,stop_words,filter , k, seed = 1234){
dtm <- dataframe %>%
  filter(upos %in% c("VERB", "NOUN", "ADV", "PROPN", "ADJ")) %>%
  # filter(paper != "bt-print.ndjson" & paper != "ekstrabladet-print.ndjson" & paper != "kristeligt-dagblad-print.ndjson") %>% 
  anti_join(stop_ord) %>%
  # Count the number of occurrences
  count(ID, lemma) %>%
  group_by(lemma) %>%
  # Filter for corpus wide frequency
  filter(sum(n) >= filter) %>%
  # Ungroup the data
  ungroup() %>%
  # Create a document term matrix
  cast_dtm(document = ID,
           term = lemma,
           value = n)

lda <- LDA(dtm, k = k, control = list(seed = seed))
return(lda)
}

tidyLDA <- function(lda, filename){
tidy_lda <- tidy(lda)
tidy_lda %>% write_csv(filename)
return(tidy_lda)
}

PlotLDA <- function(tidyLDA, n, title){
  tidyLDA %>%
  group_by(topic) %>%
  top_n(n, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)%>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = title,
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 5, scales = "free")
}

topicmodels2LDAvis <- function(x, ...){
  post <- topicmodels::posterior(x)
  if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
  mat <- x@wordassignments
  LDAvis::createJSON(
    phi = post[["terms"]], 
    theta = post[["topics"]],
    vocab = colnames(post[["terms"]]),
    doc.length = slam::row_sums(mat, na.rm = TRUE),
    term.frequency = slam::col_sums(mat, na.rm = TRUE)
  )
}


topicmodels2pyLDAvis <- function(x, ...){
  post <- topicmodels::posterior(x)
  if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
  mat <- x@wordassignments
    list = list(
    phi = post[["terms"]], 
    theta = post[["topics"]],
    vocab = colnames(post[["terms"]]),
    doc.length = slam::row_sums(mat, na.rm = TRUE),
    term.frequency = slam::col_sums(mat, na.rm = TRUE)
  )
    return(list)
}
```


```{r}
baselineLDA <- df %>%
  filter(CoronaStatus == "Baseline") %>%
  FuncLDA(stop_ord, 50 , 20, seed = 1234)

baselineTidyLDA <- baselineLDA %>% tidyLDA("baselineLDA.csv")

baselineTidyLDA %>%
  PlotLDA(20, "Top 20 terms in each topic: Pre-Outbreak")

#serVis(topicmodels2LDAvis(baselineLDA), out.dir = "/home/ptrt/BachelorsProjectCOVID19NLP/baselineLDA")

write_json(topicmodels2pyLDAvis(baselineLDA),"LDAvisModels/baselineLDAvis.json")
baselineLDAvis <- topicmodels2pyLDAvis(baselineLDA)

outbreakLDA <- df %>%
  filter(CoronaStatus == "Outbreak") %>% 
  FuncLDA(stop_ord,50 , 20, seed = 1234)

outbreakTidyLDA <- outbreakLDA %>% tidyLDA("OutbreakLDA.csv")

outbreakTidyLDA %>% PlotLDA(20,"Top 20 terms in each topic: Outbreak") %>% ggplotly()

write_json(topicmodels2pyLDAvis(outbreakLDA),"LDAvisModels/outbreakLDAvis.json")
#serVis(topicmodels2LDAvis(outbreakLDA), out.dir = "/home/ptrt/BachelorsProjectCOVID19NLP/OutbreakLDA")
outbreakLDAvis <- topicmodels2pyLDAvis(outbreakLDA)

postOutbreakLDA <- df %>%
  filter(CoronaStatus == "PostOutbreak") %>% 
  FuncLDA(stop_ord,50 , 20, seed = 1234)

postOutbreakTidyLDA <- outbreakLDA %>% tidyLDA("PostOutbreakLDA.csv")

postOutbreakTidyLDA %>%
  PlotLDA(20,"Top 20 terms in each topic: Post-Outbreak")

#serVis(topicmodels2LDAvis(outbreakLDA), out.dir = "/home/ptrt/BachelorsProjectCOVID19NLP/PostOutbreakLDA")
write_json(topicmodels2pyLDAvis(postOutbreakLDA),"LDAvisModels/postOutbreakLDAvis.json")
postOutbreakLDAvis <- topicmodels2pyLDAvis(postOutbreakLDA)
```


```{r}
py <- reticulate::py

pyLDAvis<- reticulate::import("pyLDAvis")

pybaselineLDAvis = pyLDAvis$prepare(
  topic_term_dists = baselineLDAvis$phi,
  doc_topic_dists = baselineLDAvis$theta,
  doc_lengths = baselineLDAvis$doc.length,
  vocab = baselineLDAvis$vocab,
  term_frequency = baselineLDAvis$term.frequency
)

pyLDAvis$save_html(pybaselineLDAvis, "Bachelor_shiny_app/LDAvisModels/baselineLDAvis.html")


pyOutbreakLDAvis = pyLDAvis$prepare(
  topic_term_dists = outbreakLDAvis$phi,
  doc_topic_dists = outbreakLDAvis$theta,
  doc_lengths = outbreakLDAvis$doc.length,
  vocab = outbreakLDAvis$vocab,
  term_frequency = outbreakLDAvis$term.frequency
)

pyLDAvis$save_html(pyOutbreakLDAvis, "Bachelor_shiny_app/LDAvisModels/outbreakLDAvis.html")

pyPostOutbreakLDAvis = pyLDAvis$prepare(
  topic_term_dists = postOutbreakLDAvis$phi,
  doc_topic_dists = postOutbreakLDAvis$theta,
  doc_lengths = postOutbreakLDAvis$doc.length,
  vocab = postOutbreakLDAvis$vocab,
  term_frequency = postOutbreakLDAvis$term.frequency
)

pyLDAvis$save_html(pyPostOutbreakLDAvis, "Bachelor_shiny_app/LDAvisModels/postOutbreakLDAvis.html")

```


```{r}
getVocab <- function(texts){
it = itoken(texts, progressbar = T, n_chunks = 10, preprocessor = tolower)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)

return(vocab)}
```

```{r}

df2 <-
  df %>% filter(upos %in% c("VERB", "NOUN", "ADV", "PROPN", "ADJ")) %>%
  anti_join(stop_ord) %>%
  mutate(lemma = paste(lemma, upos, sep = "_")) %> <- %
  group_by(ID) %>%
  mutate(text = glue::glue_collapse(lemma, " ")) %>% 
  select(c("ID", "date", "paper", "CoronaStatus", "text")) %>%
  unique()
```

```{r}
baselineVocab <- df2 %>% filter(date < ymd("2020-02-26") ) %>% .$text %>% getVocab()
outbreakVocab<- df2 %>% filter(date > ymd("2020-02-28") & date < ymd("2020-04-12")) %>% .$text %>% getVocab()
postOutbreakVocab <- df2 %>% filter( date > ymd("2020-04-12")) %>% .$text %>% getVocab()

totalVocab <- rbind(baselineVocab,outbreakVocab,postOutbreakVocab) %>% dplyr::select(term) %>% unique()
```

```{r}
gloveFunc <- function(texts){
it = itoken(texts, progressbar = T, n_chunks = 10, preprocessor = tolower)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)

# Use our filtered vocabulary
vectorizer = vocab_vectorizer(vocab)
# use window of 5 for context words
tcm = create_tcm(it, vectorizer, skip_grams_window = 3L)

glove = GlobalVectors$new(rank = 150, x_max = 50)
wv_main = glove$fit_transform(tcm, n_iter = 30, convergence_tol = 0.05, n_threads = 16)

wv_context = glove$components
word_vectors = wv_main + t(wv_context)
return(word_vectors)}

baselineGloVe <- df2 %>% filter(date < ymd("2020-02-26") ) %>%
  .$text %>%
  gloveFunc()

outbreakGloVe <- df2 %>%
  filter(date > ymd("2020-02-28") & date < ymd("2020-04-12")) %>%
  .$text %>%
  gloveFunc()

postOutbreakGloVe <- df2 %>%
  filter( date > ymd("2020-04-12")) %>%
  .$text %>%
  gloveFunc()
```


```{r, fig.height=15, fig.width=20}
coSimFunc <- function(word_vectors,word,top_n, CoronaStatus){
 word_vector = word_vectors[word, , drop = FALSE]
  cos_sim = sim2(
    x = word_vectors,
    y = word_vector,
    method = "cosine",
    norm = "l2"
  ) %>% as.data.frame()
cos_sim["lemma"] <- rownames(cos_sim)
cos_sim["similarity"] <- cos_sim[word] 

cos_sim<- cos_sim %>%
  arrange(desc(similarity)) %>%
  head(top_n +1) %>% 
  filter(lemma != word) %>% 
  mutate(CoronaStatus = CoronaStatus)
return(cos_sim)
}
merge_searches <- function(word,top_n = 15 , baseline = baselineGloVe, outbreak = outbreakGloVe, postOutbreak =postOutbreakGloVe) {
  
  baseline = try(baseline %>% coSimFunc(word,top_n,"Baseline"), silent = T)
  outbreak = try(outbreak %>% coSimFunc(word,top_n,"Outbreak"), silent = T)
  postOutbreak = try(postOutbreak %>% coSimFunc(word,top_n,"PostOutbreak"), silent = T)

df = rbind(baseline,outbreak,postOutbreak) %>%
  filter(CoronaStatus == "Baseline" | CoronaStatus == "Outbreak" |CoronaStatus == "PostOutbreak")
return(df)
}

```

```{r, fig.height=6, fig.width=12}
search_and_plot <- function(word,
           top_n = 15,
           baseline = baselineGloVe,
           outbreak = outbreakGloVe,
           postOutbreak = postOutbreakGloVe) {
    result = merge_searches(word, top_n, baseline, outbreak, postOutbreak) %>%
      mutate(
        CoronaStatus = as.factor(CoronaStatus),
        lemma1 = reorder_within(as.factor(lemma), as.numeric(similarity), CoronaStatus)
      )
    result %>%   ggplot(aes(lemma1, as.numeric(as.character(similarity)), fill = CoronaStatus)) +
      geom_col(show.legend = FALSE) +
      labs(x = NULL, y = "similarity") +
      coord_flip() +
      facet_wrap(~ CoronaStatus, scales = "free_y") +
      scale_x_reordered() +
      scale_y_continuous(n.breaks = 5)+
      ggtitle(paste("top", top_n, "words related to:", word))
      
}

search_and_plot2 <- function(word,
                            top_n = 15,
                            baseline = baselineGloVe,
                            outbreak = outbreakGloVe,
                            postOutbreak = postOutbreakGloVe) {
  
  baseline = try(baseline %>%
                   coSimFunc(word, top_n, "Baseline"), silent = T)
  
  outbreak = try(outbreak %>%
                   coSimFunc(word, top_n, "Outbreak"), silent = T)
  
  postOutbreak = try(postOutbreak %>%
                       coSimFunc(word, top_n, "PostOutbreak"),
silent = T
  )
  
  result = rbind(baseline, outbreak, postOutbreak) %>%
    filter(
      CoronaStatus == "Baseline" |
        CoronaStatus == "Outbreak" | CoronaStatus == "PostOutbreak"
    ) %>%
    mutate(
      CoronaStatus = as.factor(CoronaStatus),
      lemma1 = reorder_within(as.factor(lemma), as.numeric(similarity), CoronaStatus)
    )
  
  result %>%   ggplot(aes(lemma1, as.numeric(as.character(similarity)), fill = CoronaStatus)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "similarity") +
    coord_flip() +
    facet_wrap(~ CoronaStatus, scales = "free_y") +
    scale_x_reordered() +
    scale_y_continuous(n.breaks = 5) +
    ggtitle(paste("top", top_n, "words related to:", word))
  
}

search_and_plot("vaccine_noun")
```



```{r}
rm(cluster)
rm(py)
rm(pybaselineLDAvis)
rm(pyLDAvis)
rm(pyOutbreakLDAvis)
rm(test)
rm(df)
rm(df2)
rm(df1)
rm(full_df)
rm(
  cluster,
  import,
  outbreakLDA,
  outbreakVocab,
  outbreakTidyLDA,
  outbreakLDAvis,
  postOutbreakLDA,
  postOutbreakLDAvis,
  postOutbreakTidyLDA,
  postOutbreakVocab,
  baselineLDA,
  baselineLDAvis,
  baselineTidyLDA,
  baselineVocab,
  stop_ord,
  totalLemmas,
  totalLemmasNames, 
  textLemmas,
  textLemmasNames,
  pybaselineLDAvis,
  py,
  pyLDAvis,
  pyOutbreakLDAvis
)
save.image("Bachelor_shiny_app/workspace.RData")


sessioninfo::session_info()


```

